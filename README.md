# ğŸŒ³ ConVerge - Graph-Based Conversation Management

**User-controlled conversational context through interactive graph visualization**

ConVerge enables non-linear navigation of AI conversations, allowing you to branch conversations, explore alternative reasoning paths, and maintain full transparency of conversational context.

---

## âœ¨ Features

- ğŸŒ³ **Graph Visualization** - Interactive conversation tree with React Flow
- ğŸ”€ **Non-linear Navigation** - Branch from any node, explore multiple paths
- ğŸ‘ï¸ **Context Transparency** - See exact context used for each response
- ğŸ¨ **Beautiful UI** - Modern design with dark mode support

---

## ğŸš€ Quick Start

### **Prerequisites**

- Python 3.11+
- Node.js 18+
- OpenRouter API key (free at [openrouter.ai](https://openrouter.ai))

### **1. Clone & Setup**

```bash
cd conVerge
```

### **2. Configure Environment**

Create `.env` file in the root directory:

```bash
OPENROUTER_API_KEY=your-key-here
```

Get your free API key from [openrouter.ai/keys](https://openrouter.ai/keys)

### **3. Start Backend**

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start FastAPI server
uvicorn app.main:app --reload --port 8001
```

Backend running at **http://localhost:8001**
- API docs: **http://localhost:8001/docs**

### **4. Start Frontend** (new terminal)

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

Frontend running at **http://localhost:5173**

### **5. Open in Browser**

Navigate to **http://localhost:5173** and start conversing! ğŸ‰

---

## ğŸ“– How to Use

1. **Create a Conversation**
   - Click "+ New Conversation"
   - Enter a title and initial system context
   - Your first conversation appears!

2. **Ask Questions**
   - Type your query in the right panel
   - Press `Cmd/Ctrl + Enter` or click "Send Query"
   - Watch the response stream in real-time

3. **Navigate the Graph**
   - Click any node to select it
   - Ask another question to branch from that node
   - Explore different conversation paths non-linearly

4. **View Details**
   - **Left Panel**: Node details, full context, response
   - **Center Panel**: Interactive graph visualization
   - **Right Panel**: Query input and streaming responses

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI       â”‚ â† React + React Flow
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ REST API + WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend API       â”‚ â† FastAPI (Python)
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ In-Memory   â”‚   â”‚ â† Python dicts (session storage)
â”‚   â”‚ Storage     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ LLM Client  â”‚   â”‚ â† OpenRouter API
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Tech Stack**

**Frontend:**
- React 18 + TypeScript
- React Flow (graph visualization)
- TanStack Query (state management)
- Tailwind CSS (styling)
- Zustand (global state)

**Backend:**
- FastAPI (async Python framework)
- Pydantic v2 (validation)
- httpx (async HTTP client)
- WebSocket (streaming)

---

## ğŸ“ Project Structure

```
conVerge/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ conversations.py    # Conversation endpoints
â”‚   â”‚   â”‚   â””â”€â”€ nodes.py            # Node endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ llm.py              # OpenRouter client
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ models.py               # Data models
â”‚   â”‚   â”œâ”€â”€ store.py                # In-memory storage
â”‚   â”‚   â””â”€â”€ schemas.py              # API schemas
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ venv/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ GraphView.tsx       # Graph visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ CustomNode.tsx      # Node component
â”‚   â”‚   â”‚   â”œâ”€â”€ ContextPanel.tsx    # Context viewer
â”‚   â”‚   â”‚   â””â”€â”€ QueryPanel.tsx      # Query input
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useConversation.ts  # API hooks
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts     # Streaming hook
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts              # API client
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ graph.ts            # TypeScript types
â”‚   â”‚   â””â”€â”€ App.tsx                 # Main app
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ node_modules/
â”‚
â”œâ”€â”€ .env                            # Environment variables
â”œâ”€â”€ IMPLEMENTATION_PLAN.md          # Technical implementation plan
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”Œ API Endpoints

### **Conversations**

```
POST   /api/conversations              # Create conversation
GET    /api/conversations              # List all conversations
GET    /api/conversations/{id}         # Get conversation details
DELETE /api/conversations/{id}         # Delete conversation
GET    /api/conversations/{id}/graph   # Get full graph structure
POST   /api/conversations/{id}/select  # Select active node
WS     /api/conversations/{id}/stream  # Stream LLM responses
```

### **Nodes**

```
GET    /api/nodes/{id}                 # Get node details
DELETE /api/nodes/{id}                 # Delete node & descendants
GET    /api/nodes/{id}/ancestors       # Get path from root
GET    /api/nodes/{id}/children        # Get child nodes
```

Explore the full API at **http://localhost:8001/docs**

---

## âš™ï¸ Configuration

### **Backend (.env)**

```bash
# Required
OPENROUTER_API_KEY=sk-or-v1-...

# Optional
DEFAULT_MODEL=google/gemma-2-9b-it:free
CORS_ORIGINS=http://localhost:5173
```

### **Frontend (.env)**

```bash
VITE_API_URL=http://localhost:8001
```

---

## ğŸš€ Production Deployment

Deploy ConVerge for **FREE** using Vercel (frontend) and Cloudflare Tunnel (backend):

### **Backend Setup (Free with Cloudflare Tunnel)**

#### 1. Start Backend Service
```bash
# The backend runs as a systemd service
sudo systemctl start converge-dev.service
sudo systemctl enable converge-dev.service  # Auto-start on boot

# Check status
sudo systemctl status converge-dev.service
```

#### 2. Start Cloudflare Tunnel
```bash
# The tunnel runs as a systemd service
sudo systemctl start converge-tunnel.service
sudo systemctl enable converge-tunnel.service  # Auto-start on boot

# Get your public URL
./get_tunnel_url.sh
```

You'll get a free public URL like:
```
https://theft-breed-saw-foot.trycloudflare.com
```

**Note:** Quick tunnels generate a new URL each time the service restarts. For a permanent URL, set up a named tunnel with a Cloudflare account.

#### 3. Manage Services
```bash
# View backend logs
sudo journalctl -u converge-dev.service -f

# View tunnel logs
sudo journalctl -u converge-tunnel.service -f

# Restart services
sudo systemctl restart converge-dev.service
sudo systemctl restart converge-tunnel.service

# Stop services
sudo systemctl stop converge-tunnel.service
sudo systemctl stop converge-dev.service
```

### **Frontend Deployment (Vercel Website)**

#### 1. Push Code to GitHub


#### 2. Import Project to Vercel

1. Go to [vercel.com](https://vercel.com) and sign in
2. Click **"Add New Project"**
3. Import your GitHub repository
4. Select the repository (e.g., `your-username/conVerge`)

#### 3. Configure Project Settings

**IMPORTANT:** You must set the Root Directory, or the build will fail!

**Framework Preset:** Vite

**Root Directory:** `frontend` âš ï¸ **Click "Edit" button and type `frontend`** âš ï¸

**Build Command:** `npm run build` (or leave blank for Vite default)

**Output Directory:** `dist` (or leave blank for Vite default)

**Install Command:** `npm install` (or leave blank for Vite default)

**Note:** Make sure to click the **Edit** button next to Root Directory and explicitly set it to `frontend`. If this isn't set, Vercel will try to build from the repository root and fail.

#### 4. Add Environment Variable

In the **Environment Variables** section, add:

**Key:** `VITE_API_URL`
**Value:** `https://theft-breed-saw-foot.trycloudflare.com`

Click **"Add"** then **"Deploy"**

#### 5. Update CORS Settings

Add your Vercel domain to backend CORS in `.env`:
```bash
CORS_ORIGINS=https://convergecontext.vercel.app,http://localhost:5173
```

Then restart the backend:
```bash
sudo systemctl restart converge-dev.service
```

#### 6. Access Your Deployed App

Your app will be live at: **https://convergecontext.vercel.app**

#### 7. Update Environment Variables (if tunnel restarts)

When your tunnel URL changes:
1. Go to your Vercel project dashboard
2. Navigate to **Settings** â†’ **Environment Variables**
3. Edit `VITE_API_URL` with the new tunnel URL
4. Redeploy: **Deployments** â†’ Click **"..."** â†’ **"Redeploy"**

### **Complete Setup Checklist**

- [x] Backend service running (`sudo systemctl status converge-dev.service`)
- [x] Cloudflare tunnel running (`sudo systemctl status converge-tunnel.service`)
- [x] Public URL obtained (`./get_tunnel_url.sh`)
- [x] Frontend deployed to Vercel
- [x] `VITE_API_URL` set in Vercel environment variables
- [x] CORS origins updated in backend `.env`
- [x] Test the deployed app at your Vercel URL

### **Alternative: Self-Hosted Backend**

See **[DEPLOY_INSTRUCTIONS.md](DEPLOY_INSTRUCTIONS.md)** for self-hosted deployment:
- DigitalOcean Droplet ($6/month)
- Custom domain with SSL
- Nginx reverse proxy
- Permanent URLs

---

## ğŸ’¾ Data Storage

**In-Memory Storage**
- All data stored in Python dictionaries
- Fast access, zero setup
- Data resets when backend restarts
- Perfect for demos and development

**Future Options:**
- Local storage (browser)
- JSON export/import
- SQLite database
- PostgreSQL (full persistence)

---

## ğŸ†“ Free LLM Models

ConVerge uses free OpenRouter models by default:

- `google/gemma-2-9b-it:free` - Fast, capable
- `meta-llama/llama-3.1-8b-instruct:free` - Large context (128K)
- `qwen/qwen-2-7b-instruct:free` - Efficient

Upgrade to paid models for better quality and higher rate limits.

---

## ğŸ›‘ Troubleshooting

**Backend won't start?**
```bash
cd backend
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

**Frontend won't start?**
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
```

**WebSocket connection fails?**
- Check backend is running on port 8001
- Verify browser console for errors
- Ensure `.env` has correct `VITE_API_URL`


## ğŸ“š References

- **Paper**: `conVerge.tex` - Formal specification
- **Plan**: `IMPLEMENTATION_PLAN.md` - Technical implementation
- **OpenRouter**: https://openrouter.ai/docs
- **React Flow**: https://reactflow.dev
- **FastAPI**: https://fastapi.tiangolo.com

---

## ğŸ¯ What's Next?

**Planned Features:**
- [ ] Export conversations (JSON, Markdown)
- [ ] Import conversations
- [ ] Search within conversations
- [ ] Browser local storage persistence
- [ ] Multiple model selection
- [ ] Node annotations and tags
- [ ] Keyboard shortcuts
- [ ] Mobile responsive design

---

## ğŸ“ License

MIT

---

## ğŸ™ Contributing

Contributions welcome! Feel free to open issues or submit PRs.

---

**Built with â¤ï¸ using React, FastAPI, and React Flow**
